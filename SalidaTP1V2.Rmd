---
title: 'DataMining en Ciencias. Trabajo Práctico 1: Procesamiento de los datos'
author: Freiman Sebastián <sebafreiman@gmail.com>, Louzau Estefanía <eplouz@gmail.com>,
  Michalla Bárbara <barbaramichalla@gmail.com>, Ríos Leandro Martín <rios.leandromartin@gmail.com>
date: "13 de septiembre de 2016"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE,}
knitr::opts_chunk$set(echo = FALSE, message=F, warning=F)
```

## Introducción

El objetivo de esta primera parte del trabajo práctico es conocer los datos con que van a trabajar, realizar un análisis exploratorio y detectar y solucionar varios problemas que presentan los datos, para poder, en sucesivos trabajos, operarlos adecuadamente.

El trabajo realizado se organiza en las siguientes secciones.

## Preparación de los datos

A continuación se describen las técnicas y herramientas utilizadas para preparar los datos que se pretende analizar.

La herramienta principal en la que se apoya este trabajo es [R](https://www.r-project.org/), utilizándose a través del [RStudio](https://www.rstudio.com/) y varios de los paquetes que provee [CRAN](https://cran.r-project.org/) y el propio software.

### Configuración del entorno

Para comenzar, se setea el directorio de trabajo, comunmente denominado _Working Directory_ de donde se obtendrán los datasets y donde se almacenarán los resultados parciales obtenidos.

```{r Working Directory}
# setwd("C:/Users/Estefi/Downloads") # no me borres el WD, eso dejamelo a mi...
#setwd("D:/MAESTRIA/DM_Ciencias/DMCTP1-master")
#setwd("C:/Users/escalada/Documents/TP1_Ciencia2016")
#getwd()
```

El dataset describe un subconjunto de 3462 galaxias captadas en una región del cielo a través de unas 64 variables. El mismo se puede obtener desde la web de [astrostatics](http://astrostatistics.psu.edu/datasets/COMBO17.csv) 

A efectos prácticos, se descarga el mismo al directorio de trabajo.

```{r Carga de Datos}
glx <- read.csv("COMBO17.csv", header = T, stringsAsFactors = F)
```

### Análisis de los datos

Se realiza una previsualización de los datos y se revisa con qué formato R interpretó a los mismos.

```{r Previsualizacion de los datos}
# summary (glx)
# head (glx) # 
str (glx)
```

Se aprecia que los datos corresponden mayormente a variables numéricas, dado que se incluyen estimadores del tamaño de la galaxia, de su corrimiento al rojo y datos y flujos y magnitudes absolutas obtenidas a diferentes longitudes de onda.

Pero una de las variables, `e.W420FE` fue importada como tipo caracter.

```{r}
str(glx$e.W420FE)
```

Por el tipo de nomenclatura utilizada, se puede deducir que se trata de un error y que debería ser considerada como variable numérica.

Se intuye que la causa del problema podría ser la notación científica esta especificada con la letra mayúscula "E", dado que comparada con la variable `VFD`, se observa que la misma presenta la notación científica con la letra "e" minúscula.

```{r}
str(glx$VFD)
```

Por lo tanto se procede a intentar convertirla al tipo numérico.

```{r Observando los datos de la variabe en cuestion}
# Se guarda la variable forzada a numeric en un vector
variable_error <- as.numeric(glx$e.W420FE)
```

Se obtiene un Warning que indica que hubo valores que no pudo convertir, los cuales dejó como `NA` o nulos.

Se imprime un resumen para identificar cuál fue el volumen de registros que no consiguió convertir y cuáles son los valores de dichos registros.

```{r Observando los datos de la variabe en cuestion 2}
# Se pide un resumen de cuántas posiciones del vector quedaron nulas
# Con esta instrucción nos devuelve un vector con valor TRUE para aquellos que son NA
table(is.na(variable_error))

# Se observan cuáles son los datos del dataset original que corresponden a 
# las posiciones nulas del vector anterior
glx$e.W420FE[is.na(variable_error)]

```

Por simple inspección se puede apreciar que los 143 datos que no fueron convertidos a "numeric", poseen el siguiente patrón en su estructura.

> `dígito + 2 espacios + numero en notación exponencial`

El equipo consensuó que por el rango esperado de la variable, se debe tomar como criterio válido que la porción correspondiente al llamado `numero en notación exponencial` es el correcto valor de la misma.

Se aplica la función `strsplit` para dividir el campo utilizando los espacios centrales. La función devuelve una lista con el `dígito` en la primera posición y el `número_notación_exponencial` en la segunda.
Para obtener el segundo elemento de esa lista, dado que no es recomendable utilizar ciclos FOR en el lenguage R, se utiliza la función `sapply`.

La misma proviene de la familia de funciones: 

- apply
- lapply
- sapply

```{r Tratamiento de los NA}
variable_error_index <- which (is.na(variable_error)) # guardamos los índices donde se encuentran los 143 datos que no fueron convertidos a "numeric"

# # ---------- Explicacion ------------
# strsplit("a_b", "_")    # la salida es una lista con los elementos "a" y "b"
# strsplit("a$b", "\\$")  # la doble contrabarra nos pertime separar caracteres especiales
# # -----------------------------------

# strsplit(glx$e.W420FE[variable_error_index], "  ")
variable_error_lista <- strsplit(glx$e.W420FE[variable_error_index], "  ")
# variable_error_lista[143]       # aquí se encuentra la lista con el "dígito" y el  "número_notación_exponencial"
# variable_error_lista[[143]][2]  # aquí se encuentra el  "número_notación_exponencial" OJO es char

# sapply (variable_error_lista, "[[", 2) 
# Le pedimos a la funcion que nos devuelva el elemento 2, de cada elemento de la lista. Esto de cada elemento de la lista se hace  con: "[["

variable_error_datos <- sapply (variable_error_lista, "[[", 2)
```

Una vez obtenido el dato para la variable, se repite el procedimiento de conversión a `numeric`, del cual ya no se obtienen errores o warnings.

Por lo tanto se procede a reemplazar los datos originales de la variable por los datos transformados, para poder operarlo correctamente en los sucesivos análisis, comprobándose que ahora los datos son del tipo `numeric` y no se observan valores `NA`.

```{r Tratamiento de los NA - IV}

# typeof(variable_error_datos) # debemos recordar que el 2do elemento de la lista es tipo char
variable_error_datosnum <- as.numeric(variable_error_datos)

# # ---------- Explicacion ------------------
# # Que hubiera sucedido con: lapply
# variable_error_datos2 <- lapply (variable_error_lista, "[[", 2)
# typeof(variable_error_datos2)
# # lapply te devuelve una lista mientras que sapply te devuelve una vector (sapply de simplify)
# # ----------------------------------------

# Reemplazamos los datos de la var en el dataset original
glx$e.W420FE[variable_error_index] <- variable_error_datosnum

# Transformamos la variable a numeric
glx$e.W420FE <- as.numeric(glx$e.W420FE)

# Vemos el tipo de datos que quedó en el dataset.
str(glx$e.W420FE)

# Para saber si después de la conversión quedaron NAs
table(is.na(glx$e.W420FE))
```


## Tratamiento de Outliers

Con el objetivo de identificar la presencia de valores extremos, se realizan controles mediante la observación de tablas y gráficos exploratorios que ayuden a vizualizarlos.

**RMag** 

Se comienza analizando las variables `Rmag` y `e.Rmag`, que representan la magnitud total en la banda R y su error de medición. 

Se analiza la variable `Rmag` por medio de un histograma y un gráfico de barras de error de `e.Rmag`.

```{r variable RMag}
library(ggplot2)
#install.packages("ggplot2")

# glx_SinOutliers <- glx [1:10, ]
glx_SinOutliers <- glx

hist (glx_SinOutliers$Rmag, main= "Histograma de Rmag", xlab="Rmag")
```

Uno de los problemas observados al estudiar los histogramas son los rangos o la cantidad de clases de la variable, los cuales son deteminados de forma empírica. Otro inconveniente es que el ancho de clase debe tenerse en consideración para distribuir las observaciones. Pero debe hacerse especial hincapié en el rango desde y hasta dónde se desea visualizar. 

En oposición a los anteriores, suelen ser preferibles los gráficos de densidad de kernel, los cuales son una función continua y pueden someterse a una derivada. Además, son muy recomendables cuando se tienen muchos registros.

Por lo tanto, realizamos un gráfico simple de densidad de kernel.

```{r Gráficos - II}
plot (density(glx_SinOutliers$ApDRmag), main="Densidad de kernel")
```

Adicionalmente, se grafica un gráfico de barras de error, a través del cual se espera tener una mejor visualización de los datos.

```{r}

plot(glx_SinOutliers$Nr, glx_SinOutliers$Rmag,
    ylim=range(c(glx_SinOutliers$Rmag-glx_SinOutliers$e.Rmag, glx_SinOutliers$Rmag+glx_SinOutliers$e.Rmag)),
    pch=19, xlab="Nro Id Galaxia", ylab="Rmag +/- Error_Incertidumbre",
    main="Grafico barras de Error_Incertidumbre")

# no tengo bien claro que es esto pero funca
# hack: we draw arrows but with very special "arrowheads"
arrows(glx_SinOutliers$Nr, glx_SinOutliers$Rmag-glx_SinOutliers$e.Rmag,glx_SinOutliers$Nr, glx_SinOutliers$Rmag+glx_SinOutliers$e.Rmag, length=0.05, angle=90, code=3)

```

Dada la gran cantidad de registros, es díficil visualizar el comportamiento de los mismos.
Se procede a realizar el mismo gráficos pero tomado una muestra al azar de 100 registros.

```{r}

glx_errorbars <- glx_SinOutliers[sample (c(1:dim(glx_SinOutliers)[1]),100), ]

plot(glx_errorbars$Nr, glx_errorbars$Rmag, ylim=range(c(glx_errorbars$Rmag-glx_errorbars$e.Rmag, glx_errorbars$Rmag+glx_errorbars$e.Rmag)),
    pch=19, xlab="Nro Id Galaxia", ylab="Rmag +/- Error_Incertidumbre",
    main="Grafico barras de Error_Incertidumbre")
# no tengo bien claro que es esto pero funca
# hack: we draw arrows but with very special "arrowheads"
arrows(glx_errorbars$Nr, glx_errorbars$Rmag-glx_errorbars$e.Rmag,glx_errorbars$Nr, glx_errorbars$Rmag+glx_errorbars$e.Rmag, length=0.05, angle=90, code=3)

```


A partir de los gráficos expuestos se puede suponer la presencia de outliers.

Luego, se presenta un gráfico tipo boxplot para visualizar la distribución de la variable.

```{r}

boxplot(glx_SinOutliers$Rmag)

#ggplot(glx_SinOutliers, aes( x = Rmag) ) + geom_histogram( aes(y = ..density..), color = "black", fill = "beige")+geom_density( kernel = "epanechnikov", color="red")
```

El boxplot presenta de manera gráfica el valor de la media, 1er y 3er cuartil y los (supuestos) outliers. Pero debe tenerse en consideración que el diagrama de cajas fue pensado para trabajar con datos en los años 70.
Hoy en día, los grandes volumenes de datos que se manejan hacen más presente la idea de poder tener (supuestos) outliers.

Sin embargo, a partir de las observaciones realizadas con todo el análisis anterior, se determina que los valores extremos para la variable en estudio podrían ser aquellos mayores o iguales a 20.

Por lo tanto, se procede a quitar las observaciones que toman valores en este rango y se vuelve a generar un gráfico de barras que muestra la distribución de la variable sin valores extremos.

```{r}
index_kill <- which(glx_SinOutliers$Rmag < 20)
glx_SinOutliers <- glx_SinOutliers[-index_kill, ]

ggplot(glx_SinOutliers, aes( x = Rmag) ) + geom_histogram( aes(y = ..density..), color = "black", fill = "beige")+geom_density( kernel = "epanechnikov", color="red")

```

**ApDRMag**

Pasemos a analizar la segunda variable de interés, `ApDRmag`, que representa la diferencia entre la magnitud total y la magnitud de apertura medidas en la banda R. Es un estimador del tamaño de una galaxia. Un valor de cero indica que es una fuente puntual de luz. Los valores negativos se generan por errores de medición.

Se grafica mediante un boxplot y un histograma:

```{r Gráficos - }

boxplot(glx_SinOutliers$ApDRmag)
```

El histograma se crea con la librería `ggplot`[^1]

```{r}
ggplot(glx_SinOutliers, aes( x = ApDRmag) ) + geom_histogram( aes(y = ..density..), color = "black", fill = "beige")+geom_density( kernel = "epanechnikov", color="red")
```

[1] Como se puede apreciar, la librería ggplot es mas vistosa para realizar gráficos. Los parámetros de "aes" están relacionados con la estética por _aesthetics_, de las variables que se quieren representar.
Luego `+ geom_histogram()` propone la forma en la que se representan los datos. Los signos `+` permiten generar los gráficos en capas; en este caso la segunda capa esta referida `+ geom_density()`
Cabe también destacar que la nomenclatura utilizada: `x = ApDRmag` en vez de: `x = glx$ApDRmag`
es consistente con la filosofía "Tidy Data" de su creador, Hadley Wickham, que menciona en su paper publicado en "The Journal of Statistical Software, vol. 59, 2014"


```{r}
index_kill <- c(which(glx_SinOutliers$ApDRmag < 0), which(glx_SinOutliers$ApDRmag > 2))
#unique (index_kill)

glx_SinOutliers <- glx_SinOutliers[-index_kill, ]


ggplot(glx_SinOutliers, aes( x = ApDRmag) ) + geom_histogram( aes(y = ..density..), color = "black", fill = "beige")+geom_density( kernel = "epanechnikov", color="red")

```

## Otros gráficos y técnicas de diagnóstico complementarias

Para el análisis de los datos de las bandas espectrales, se realizarán scatter graphs donde la abcisa será la banda espectral de casa sistema fotométrico y la ordenada estará normalizada respecto de la magnitud absoluta en el ultravioleta a 280 nm `S280MAG`. La normalización se hace restando porque estas variables son logarítmicas.

```{r Gráficos - V}

# install.packages("gridExtra") 
library(gridExtra)
p1 <- qplot(BjMAG, S280MAG-BjMAG, data = glx)
p2 <- qplot(BbMAG, S280MAG-BbMAG, data = glx)
p3 <- qplot(BbMAG, S280MAG-BbMAG, data = glx)
grid.arrange(p1, p2, p3, ncol=3)
```

Es fácil apreciar que existe un valos en `BjMAG` muy alejado de la nube de puntos. Procedemos a eliminarlo.

```{r Gráficos - Vb}
which.max(glx$BjMAG) 
#glx[2,1:13]
glx$BjMAG[2]

p1<-qplot(BjMAG,S280MAG-BjMAG, data = glx_SinOutliers)
p2 <- qplot(gsMAG,S280MAG-gsMAG, data = glx_SinOutliers)
p3 <- qplot(BbMAG, S280MAG-BbMAG, data = glx_SinOutliers)
grid.arrange(p1, p2, p3, ncol=3)

```

Luego de la remoción, los tres gráficos presentan la similitud esperada en la distribución.

Bajo la misma técnica se analizan las otras bandas espectrales.

```{r Gráficos - VII}

p4<-qplot(UjMAG,S280MAG-UjMAG, data = glx_SinOutliers)
p5 <-qplot(usMAG,S280MAG-usMAG, data = glx_SinOutliers)
p6 <- qplot(UbMAG,S280MAG-UbMAG, data = glx_SinOutliers)
grid.arrange(p4, p5, p6, ncol=3)
```

Se procede a elimimar los puntos anómalos. En este caso se tomó el criterio que las bandas normalizadas se deben encontrar dentro del intervalo [-3;+3]

```{r Gráficos - VIIb}
which.max(glx_SinOutliers$S280MAG-glx_SinOutliers$UjMAG) 
which.max(glx_SinOutliers$S280MAG-glx_SinOutliers$usMAG)
which.min(glx_SinOutliers$UjMAG)
which.min(glx_SinOutliers$usMAG)

# como saco los cuatro menores de S280MAG-UbMAG
index_kill <- c (which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$UjMAG) > 3), which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$usMAG) > 3),which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$UbMAG) > 3))
unique (index_kill)

glx_SinOutliers <- glx_SinOutliers[-index_kill, ]


p4<-qplot(UjMAG,S280MAG-UjMAG, data = glx_SinOutliers)
p5 <-qplot(usMAG,S280MAG-usMAG, data = glx_SinOutliers)
p6 <- qplot(UbMAG,S280MAG-UbMAG, data = glx_SinOutliers)
grid.arrange(p4, p5, p6, ncol=3)


```

Se vuelve a generar el gráfico y por inspección visual se eliminan los puntos anómalos.

```{r Gráficos - VIII}

p7<-qplot(VjMAG,S280MAG-VjMAG, data = glx_SinOutliers)
p8 <-qplot(rsMAG,S280MAG-rsMAG, data = glx_SinOutliers)
p9 <- qplot(VnMAG,S280MAG-VnMAG, data = glx_SinOutliers)
grid.arrange(p7, p8, p9, ncol=3)

index_kill <- c (which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$VjMAG) > 3.5), which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$rsMAG) > 3.5),which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$VnMAG) > 3.5))
unique (index_kill)

glx_SinOutliers <- glx_SinOutliers[-index_kill, ]

p7<-qplot(VjMAG,S280MAG-VjMAG, data = glx_SinOutliers)
p8 <-qplot(rsMAG,S280MAG-rsMAG, data = glx_SinOutliers)
p9 <- qplot(VnMAG,S280MAG-VnMAG, data = glx_SinOutliers)
grid.arrange(p7, p8, p9, ncol=3)

which.min(glx_SinOutliers$S280MAG - glx_SinOutliers$rsMAG)
glx_SinOutliers$S280MAG[which.min(glx_SinOutliers$S280MAG - glx_SinOutliers$rsMAG)] - glx_SinOutliers$rsMAG[which.min(glx_SinOutliers$S280MAG - glx_SinOutliers$rsMAG)]


index_kill <- c (which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$VjMAG) > 3.3), which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$rsMAG) > 3.2),which(abs(glx_SinOutliers$S280MAG-glx_SinOutliers$VnMAG) > 3.2))
unique (index_kill)

glx_SinOutliers <- glx_SinOutliers[-index_kill, ]

```

Finalmente se llega a distribuciones de puntos similares para los distintos sistemas fotométricos en las distintas bandas espectrales.

```{r Gráficos - VIIIb}

p7<-qplot(VjMAG,S280MAG-VjMAG, data = glx_SinOutliers)
p8 <-qplot(rsMAG,S280MAG-rsMAG, data = glx_SinOutliers)
p9 <- qplot(VnMAG,S280MAG-VnMAG, data = glx_SinOutliers)
grid.arrange(p7, p8, p9, ncol=3)

```

## Tratamiento de Datos Faltantes

Se verifica la presencia de datos faltantes en cualquiera de las variables del dataset.

El resultado indica que hay datos faltantes en 50 registros y que los mismos se distribuyen en 4 variables, como sigue:

```{r}
glx_SinOutliersNas <- glx_SinOutliers

table(is.na(glx_SinOutliers))

colnames(glx_SinOutliers)[apply(glx_SinOutliers,2,anyNA)]

indexfilas_NA <- as.numeric(rownames(glx_SinOutliers[rowSums(is.na(glx_SinOutliers)) > 0,]))
```

El equipo considera que la cantidad de casos con datos faltantes es despreciable, por lo que se procede a eliminar dichas observaciones para asegurar que se trabajará con datos completos y sin rellenar.

Se comprueba que luego de retirados dichos casos, el dataset obtenido ya no posee datos faltantes:

```{r}
glx_SinOutliersNas <- glx_SinOutliers[-(indexfilas_NA),] 

table(is.na(glx_SinOutliersNas))

```


## Correlación entre variables

A continuación se realiza el análisis de correlación utilizando únicamente las variables que representan las bandas espectrales de las galaxias para diferentes sistemas de fotometría.
Para ello, se combinan diferentes versiones del dataset para comprender su impacto en el estudio de la correlación.

En primer lugar, se muestra la matriz de correlación para los datos con valores nulos (NAs)

```{r Armamos las matrices de correlaciones - II}
espectrales <- c(10,12,14,16,18,20,22,24,26,28)

#head( glx_SinOutliers[, espectrales] )

DfMagnitudes <- glx_SinOutliers[, espectrales]

Cor_DfMagnitudes <- cor(DfMagnitudes, y = NULL, use = "everything",  method = "pearson")
Cor_DfMagnitudes
#esto esta bueno mostrar que la correlacion te queda con NAs
# y se mejora con el pairwise
# dale porfis mostremoslo  :) te pusiste chinchuda por el NA o por el cambio de clase
```

Luego se recalcula la matriz de correlación pero esta vez quitando los valores nulos mediante la técnica _pairwise_, que alternará entre filas y columnas retirando aquellas que en cada variable presenten valores nulos.

```{r}
Cor_DfMagnitudes <- cor(DfMagnitudes, y = NULL, use = "pairwise.complete.obs",  method = "pearson")
Cor_DfMagnitudes
```

Finalmente, se muestra la misma matriz pero para los datos normalizados restando la variable `S280Mag`

```{r}
# normalizo restando la var S280MAG
DfMagnitudesNorm <-  DfMagnitudes$S280MAG - DfMagnitudes

Cor_DfMagnitudesNorm <- cor(DfMagnitudesNorm, y = NULL, use = "pairwise.complete.obs",  method = "pearson")
Cor_DfMagnitudesNorm

# estaria muy bueno hacerlo tipo funcion que vaya de 1 a 7 
# y acada variable lea el nombre le agregue "_Norm"

```

Luego, se presentan los gráficos tipo _heatmap_ para las matrices sin normalizar y normalizadas, a modo de ofrecer una comparativa visual del efecto de la última por sobre los resultados obtenidos en la anterior.

```{r Armamos las matrices de correlaciones - III}

library(ggplot2)
library(reshape2)

# heatmap de corr sin normalizar
qplot(x=Var1, y=Var2, data=melt(cor(DfMagnitudes, use="p")), fill=value, geom="tile", main="Correlograma con datos sin normalizar") +
   scale_fill_gradient2(limits=c(-1, 1)) 


# heatmap de corr sin normalizar
qplot(x=Var1, y=Var2, data=melt(cor(DfMagnitudesNorm[,1:9], use="p")), fill=value, geom="tile", main="Correlograma con datos normalizados") +
   scale_fill_gradient2(limits=c(-1, 1))
```

Se puede observar que, normalizando por la variable de la magnitud absoluta, aquellas variables que presentan mayor correlación se separan del resto.